{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Assessment - Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook provides a comprehensive analysis of the German Credit Dataset focusing on understanding the credit risk assessment problem where it's **5 times more expensive to classify an unworthy customer as creditworthy** than vice versa.\n",
    "\n",
    "## Dataset Overview\n",
    "The German Credit Dataset contains 1000 instances with 20 features for credit risk assessment. The target variable indicates:\n",
    "- 1 = creditworthy\n",
    "- 2 = not creditworthy\n",
    "\n",
    "## Cost Matrix\n",
    "| Actual \\ Predicted | Creditworthy (1) | Not Creditworthy (2) |\n",
    "|-------------------|------------------|----------------------|\n",
    "| Creditworthy (1)  | 0                | 1                    |\n",
    "| Not Creditworthy (2) | **5**         | 0                    |\n",
    "\n",
    "This means misclassifying a bad customer as good costs 5 times more than misclassifying a good customer as bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature names based on German Credit Dataset documentation\n",
    "feature_names = [\n",
    "    'checking_account',    # Status of existing checking account\n",
    "    'duration',           # Duration in months\n",
    "    'credit_history',     # Credit history\n",
    "    'purpose',            # Purpose of credit\n",
    "    'credit_amount',      # Credit amount\n",
    "    'savings_account',    # Savings account/bonds\n",
    "    'employment',         # Present employment since\n",
    "    'installment_rate',   # Installment rate in percentage of disposable income\n",
    "    'personal_status',    # Personal status and sex\n",
    "    'other_debtors',      # Other debtors/guarantors\n",
    "    'residence_since',    # Present residence since\n",
    "    'property',           # Property\n",
    "    'age',               # Age in years\n",
    "    'other_installments', # Other installment plans\n",
    "    'housing',           # Housing\n",
    "    'existing_credits',   # Number of existing credits at this bank\n",
    "    'job',               # Job\n",
    "    'dependents',        # Number of people liable to provide maintenance for\n",
    "    'telephone',         # Telephone\n",
    "    'foreign_worker',    # Foreign worker\n",
    "    'target'             # Target variable (1=good, 2=bad)\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('kredit.dat', sep='\\t', header=None, names=feature_names)\n",
    "\n",
    "# Replace '?' with NaN for proper missing value handling\n",
    "data = data.replace('?', np.nan)\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"Dataset Info:\")\n",
    "data.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Missing Values Count:\")\n",
    "missing_counts = data.isnull().sum()\n",
    "missing_percentage = (missing_counts / len(data)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Basic Statistics:\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target distribution\n",
    "target_counts = data['target'].value_counts().sort_index()\n",
    "target_percentages = data['target'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(f\"Creditworthy (1): {target_counts[1]} ({target_percentages[1]:.1f}%)\")\n",
    "print(f\"Not Creditworthy (2): {target_counts[2]} ({target_percentages[2]:.1f}%)\")\n",
    "\n",
    "# Calculate class imbalance ratio\n",
    "imbalance_ratio = target_counts[1] / target_counts[2]\n",
    "print(f\"\\nClass Imbalance Ratio (Good:Bad): {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "# Visualize target distribution with cost implications\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Target distribution\n",
    "labels = ['Creditworthy\\n(Class 1)', 'Not Creditworthy\\n(Class 2)']\n",
    "colors = ['lightgreen', 'lightcoral']\n",
    "ax1.pie(target_counts.values, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Target Variable Distribution')\n",
    "\n",
    "# Cost implications visualization\n",
    "cost_matrix = np.array([[0, 1], [5, 0]])\n",
    "sns.heatmap(cost_matrix, annot=True, fmt='d', cmap='Reds', \n",
    "           xticklabels=['Pred: Good', 'Pred: Bad'], \n",
    "           yticklabels=['True: Good', 'True: Bad'], ax=ax2)\n",
    "ax2.set_title('Cost Matrix\\n(5x more expensive to misclassify bad as good)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate expected cost for naive strategies\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Expected Costs for Naive Strategies:\")\n",
    "print(f\"Always predict 'Good': Cost = {target_counts[2] * 5} (misclassify {target_counts[2]} bad customers)\")\n",
    "print(f\"Always predict 'Bad': Cost = {target_counts[1] * 1} (misclassify {target_counts[1]} good customers)\")\n",
    "print(f\"Random prediction would have expected cost around: {(target_counts[1] * 5 * 0.3 + target_counts[2] * 1 * 0.7):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical features\n",
    "numerical_features = ['duration', 'credit_amount', 'installment_rate', 'residence_since', 'age', \n",
    "                     'existing_credits', 'dependents']\n",
    "categorical_features = [col for col in data.columns if col not in numerical_features and col != 'target']\n",
    "\n",
    "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# Convert numerical columns to numeric (in case they're stored as strings)\n",
    "for col in numerical_features:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numerical feature distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    if i < len(axes):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Handle missing values for visualization\n",
    "        feature_data = data[feature].dropna()\n",
    "        \n",
    "        # Histogram\n",
    "        ax.hist(feature_data, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax.set_title(f'{feature.replace(\"_\", \" \").title()}\\nMean: {feature_data.mean():.1f}, Std: {feature_data.std():.1f}')\n",
    "        ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "        ax.set_ylabel('Frequency')\n",
    "        \n",
    "        # Add statistics\n",
    "        ax.axvline(feature_data.mean(), color='red', linestyle='--', alpha=0.8, label=f'Mean: {feature_data.mean():.1f}')\n",
    "        ax.axvline(feature_data.median(), color='orange', linestyle='--', alpha=0.8, label=f'Median: {feature_data.median():.1f}')\n",
    "        ax.legend()\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(numerical_features), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribution of Numerical Features', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical feature distributions\n",
    "n_categorical = len(categorical_features)\n",
    "n_cols = 3\n",
    "n_rows = (n_categorical + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 4 * n_rows))\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    if i < len(axes):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Count values including missing\n",
    "        feature_counts = data[feature].fillna('Missing').value_counts()\n",
    "        \n",
    "        # Bar plot\n",
    "        feature_counts.plot(kind='bar', ax=ax, color='lightblue', alpha=0.7)\n",
    "        ax.set_title(f'{feature.replace(\"_\", \" \").title()}\\nUnique Values: {data[feature].nunique()}')\n",
    "        ax.set_xlabel('Categories')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value counts on bars\n",
    "        for j, (category, count) in enumerate(feature_counts.items()):\n",
    "            ax.text(j, count + max(feature_counts) * 0.01, str(count), \n",
    "                   ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(categorical_features), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribution of Categorical Features', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Relationship Analysis with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features vs target - Box plots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    if i < len(axes):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create box plot\n",
    "        data_for_plot = data[[feature, 'target']].dropna()\n",
    "        \n",
    "        box_data = [data_for_plot[data_for_plot['target'] == 1][feature],\n",
    "                   data_for_plot[data_for_plot['target'] == 2][feature]]\n",
    "        \n",
    "        bp = ax.boxplot(box_data, labels=['Creditworthy\\n(1)', 'Not Creditworthy\\n(2)'],\n",
    "                       patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightgreen')\n",
    "        bp['boxes'][1].set_facecolor('lightcoral')\n",
    "        \n",
    "        ax.set_title(f'{feature.replace(\"_\", \" \").title()} by Credit Risk')\n",
    "        ax.set_ylabel(feature.replace('_', ' ').title())\n",
    "        \n",
    "        # Add statistical test\n",
    "        good_data = data_for_plot[data_for_plot['target'] == 1][feature]\n",
    "        bad_data = data_for_plot[data_for_plot['target'] == 2][feature]\n",
    "        \n",
    "        if len(good_data) > 0 and len(bad_data) > 0:\n",
    "            stat, p_value = stats.mannwhitneyu(good_data, bad_data, alternative='two-sided')\n",
    "            ax.text(0.02, 0.98, f'Mann-Whitney U p-value: {p_value:.4f}', \n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(numerical_features), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Numerical Features vs Credit Risk', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs target - Contingency tables and chi-square tests\n",
    "print(\"Categorical Features vs Target - Statistical Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "chi_square_results = []\n",
    "\n",
    "for feature in categorical_features[:6]:  # Show first 6 to avoid too much output\n",
    "    print(f\"\\n{feature.replace('_', ' ').title()}:\")\n",
    "    \n",
    "    # Create contingency table\n",
    "    contingency_table = pd.crosstab(data[feature].fillna('Missing'), data['target'], margins=True)\n",
    "    print(contingency_table)\n",
    "    \n",
    "    # Chi-square test (exclude margins)\n",
    "    contingency_no_margins = contingency_table.iloc[:-1, :-1]\n",
    "    if contingency_no_margins.shape[0] > 1 and contingency_no_margins.shape[1] > 1:\n",
    "        chi2, p_value, dof, expected = chi2_contingency(contingency_no_margins)\n",
    "        print(f\"Chi-square statistic: {chi2:.4f}, p-value: {p_value:.4f}\")\n",
    "        \n",
    "        chi_square_results.append({\n",
    "            'Feature': feature,\n",
    "            'Chi2': chi2,\n",
    "            'p_value': p_value,\n",
    "            'Significant': p_value < 0.05\n",
    "        })\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Summary of chi-square results\n",
    "if chi_square_results:\n",
    "    chi_df = pd.DataFrame(chi_square_results)\n",
    "    print(\"\\nSummary of Chi-square Test Results:\")\n",
    "    print(chi_df.sort_values('Chi2', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top categorical features vs target\n",
    "top_categorical = ['checking_account', 'credit_history', 'savings_account', 'employment', 'purpose', 'housing']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(top_categorical):\n",
    "    if i < len(axes):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create stacked bar chart\n",
    "        cross_tab = pd.crosstab(data[feature].fillna('Missing'), data['target'], normalize='index') * 100\n",
    "        \n",
    "        cross_tab.plot(kind='bar', stacked=True, ax=ax, \n",
    "                      color=['lightgreen', 'lightcoral'],\n",
    "                      legend=False if i != 0 else True)\n",
    "        \n",
    "        ax.set_title(f'{feature.replace(\"_\", \" \").title()} vs Credit Risk\\n(Percentage within each category)')\n",
    "        ax.set_xlabel('Categories')\n",
    "        ax.set_ylabel('Percentage')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.legend(['Creditworthy (1)', 'Not Creditworthy (2)'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Categorical Features vs Credit Risk (Normalized)', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation and Association Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "numerical_data = data[numerical_features + ['target']]\n",
    "correlation_matrix = numerical_data.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.3f', ax=ax)\n",
    "ax.set_title('Correlation Matrix - Numerical Features and Target')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlations with target\n",
    "print(\"Correlations with Target Variable:\")\n",
    "target_correlations = correlation_matrix['target'].drop('target').sort_values(key=abs, ascending=False)\n",
    "print(target_correlations)\n",
    "\n",
    "print(f\"\\nStrongest positive correlation with bad credit: {target_correlations.idxmax()} ({target_correlations.max():.3f})\")\n",
    "print(f\"Strongest negative correlation with bad credit: {target_correlations.idxmin()} ({target_correlations.min():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association analysis for key categorical features\n",
    "# Calculate Cramér's V for categorical variables\n",
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\" Calculate Cramér's V statistic for categorical-categorical association.\"\"\"\n",
    "    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n",
    "\n",
    "print(\"Association Strength (Cramér's V) between Categorical Features:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate associations between key categorical features\n",
    "key_categorical = ['checking_account', 'credit_history', 'savings_account', 'employment']\n",
    "association_matrix = np.zeros((len(key_categorical), len(key_categorical)))\n",
    "\n",
    "for i, feature1 in enumerate(key_categorical):\n",
    "    for j, feature2 in enumerate(key_categorical):\n",
    "        if i != j:\n",
    "            # Create contingency table\n",
    "            contingency = pd.crosstab(data[feature1].fillna('Missing'), \n",
    "                                    data[feature2].fillna('Missing'))\n",
    "            if contingency.shape[0] > 1 and contingency.shape[1] > 1:\n",
    "                association_matrix[i, j] = cramers_v(contingency.values)\n",
    "        else:\n",
    "            association_matrix[i, j] = 1.0\n",
    "\n",
    "# Plot association matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(association_matrix, annot=True, cmap='viridis', \n",
    "           xticklabels=[f.replace('_', ' ').title() for f in key_categorical],\n",
    "           yticklabels=[f.replace('_', ' ').title() for f in key_categorical],\n",
    "           square=True, fmt='.3f', ax=ax)\n",
    "ax.set_title('Association Matrix (Cramér\\'s V)\\nCategorical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cost-Sensitive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassification costs and their implications\n",
    "def calculate_expected_cost(true_labels, predicted_labels, cost_matrix=np.array([[0, 1], [5, 0]])):\n",
    "    \"\"\"Calculate expected cost given true labels, predictions, and cost matrix.\"\"\"\n",
    "    # Convert to 0-1 indexing\n",
    "    true_binary = (true_labels == 2).astype(int)\n",
    "    pred_binary = (predicted_labels == 2).astype(int)\n",
    "    \n",
    "    total_cost = 0\n",
    "    for true_val, pred_val in zip(true_binary, pred_binary):\n",
    "        total_cost += cost_matrix[true_val, pred_val]\n",
    "    \n",
    "    return total_cost\n",
    "\n",
    "# Calculate costs for different prediction strategies\n",
    "n_samples = len(data)\n",
    "n_good = sum(data['target'] == 1)\n",
    "n_bad = sum(data['target'] == 2)\n",
    "\n",
    "strategies = {\n",
    "    'Always Predict Good': {\n",
    "        'predictions': np.ones(n_samples),\n",
    "        'description': 'Classify everyone as creditworthy'\n",
    "    },\n",
    "    'Always Predict Bad': {\n",
    "        'predictions': np.ones(n_samples) * 2,\n",
    "        'description': 'Classify everyone as not creditworthy'\n",
    "    },\n",
    "    'Random (70% Good)': {\n",
    "        'predictions': np.random.choice([1, 2], size=n_samples, p=[0.7, 0.3]),\n",
    "        'description': 'Random classification with 70% good'\n",
    "    },\n",
    "    'Perfect Classifier': {\n",
    "        'predictions': data['target'].values,\n",
    "        'description': 'Perfect predictions (baseline cost = 0)'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Cost Analysis for Different Prediction Strategies:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "strategy_results = []\n",
    "for strategy_name, strategy_info in strategies.items():\n",
    "    cost = calculate_expected_cost(data['target'].values, strategy_info['predictions'])\n",
    "    cost_per_sample = cost / n_samples\n",
    "    \n",
    "    strategy_results.append({\n",
    "        'Strategy': strategy_name,\n",
    "        'Total Cost': cost,\n",
    "        'Cost per Sample': cost_per_sample,\n",
    "        'Description': strategy_info['description']\n",
    "    })\n",
    "    \n",
    "    print(f\"{strategy_name}:\")\n",
    "    print(f\"  Total Cost: {cost}\")\n",
    "    print(f\"  Cost per Sample: {cost_per_sample:.3f}\")\n",
    "    print(f\"  Description: {strategy_info['description']}\")\n",
    "    print()\n",
    "\n",
    "# Visualize strategy costs\n",
    "strategy_df = pd.DataFrame(strategy_results)\n",
    "strategy_df = strategy_df.sort_values('Total Cost')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Total cost comparison\n",
    "bars1 = ax1.bar(range(len(strategy_df)), strategy_df['Total Cost'], \n",
    "                color=['green', 'blue', 'orange', 'red'])\n",
    "ax1.set_xlabel('Strategy')\n",
    "ax1.set_ylabel('Total Cost')\n",
    "ax1.set_title('Total Cost by Prediction Strategy')\n",
    "ax1.set_xticks(range(len(strategy_df)))\n",
    "ax1.set_xticklabels(strategy_df['Strategy'], rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars1):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "             f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "# Cost breakdown visualization\n",
    "cost_breakdown = pd.DataFrame({\n",
    "    'False Positive (Good→Bad)': [n_good * 1, 0, n_good * 0.3, 0],\n",
    "    'False Negative (Bad→Good)': [0, n_bad * 5, n_bad * 0.7 * 5, 0]\n",
    "}, index=['Always Good', 'Always Bad', 'Random', 'Perfect'])\n",
    "\n",
    "cost_breakdown.plot(kind='bar', stacked=True, ax=ax2, color=['orange', 'red'])\n",
    "ax2.set_title('Cost Breakdown by Error Type')\n",
    "ax2.set_xlabel('Strategy')\n",
    "ax2.set_ylabel('Cost')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.legend(title='Error Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact of class imbalance on total cost\n",
    "print(\"Impact of Class Imbalance on Cost-Sensitive Learning:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate the break-even point for classification threshold\n",
    "# At what probability threshold should we classify as \"good\"?\n",
    "cost_fp = 1  # Cost of false positive (classify bad as good)\n",
    "cost_fn = 5  # Cost of false negative (classify good as bad)\n",
    "prior_good = n_good / n_samples\n",
    "prior_bad = n_bad / n_samples\n",
    "\n",
    "# Break-even threshold: P(good|x) > cost_fp / (cost_fp + cost_fn)\n",
    "break_even_threshold = cost_fp / (cost_fp + cost_fn)\n",
    "\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  Good customers: {n_good} ({prior_good:.3f})\")\n",
    "print(f\"  Bad customers: {n_bad} ({prior_bad:.3f})\")\n",
    "print(f\"\\nCost matrix:\")\n",
    "print(f\"  Cost of FP (classify bad as good): {cost_fp}\")\n",
    "print(f\"  Cost of FN (classify good as bad): {cost_fn}\")\n",
    "print(f\"\\nOptimal decision threshold:\")\n",
    "print(f\"  Classify as 'good' if P(good|features) > {break_even_threshold:.3f}\")\n",
    "print(f\"  This means we need {break_even_threshold*100:.1f}% confidence to classify as good\")\n",
    "\n",
    "# Sensitivity analysis: how does cost change with different thresholds?\n",
    "thresholds = np.linspace(0, 1, 21)\n",
    "expected_costs = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Simulate predictions based on threshold\n",
    "    # Assume we have some probability estimates (use random for illustration)\n",
    "    np.random.seed(42)\n",
    "    probabilities = np.random.beta(2, 3, n_samples)  # Skewed towards lower probabilities\n",
    "    predictions = (probabilities > threshold).astype(int) + 1  # Convert to 1,2 scale\n",
    "    \n",
    "    cost = calculate_expected_cost(data['target'].values, predictions)\n",
    "    expected_costs.append(cost)\n",
    "\n",
    "# Plot threshold sensitivity\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(thresholds, expected_costs, 'b-', linewidth=2, marker='o')\n",
    "plt.axvline(x=break_even_threshold, color='red', linestyle='--', \n",
    "           label=f'Theoretical optimal: {break_even_threshold:.3f}')\n",
    "plt.xlabel('Classification Threshold (Probability to classify as Good)')\n",
    "plt.ylabel('Expected Total Cost')\n",
    "plt.title('Cost Sensitivity Analysis: Impact of Classification Threshold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMinimum expected cost occurs at threshold: {thresholds[np.argmin(expected_costs)]:.3f}\")\n",
    "print(f\"Minimum expected cost: {min(expected_costs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of missing values\n",
    "print(\"Detailed Missing Value Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Features with missing values\n",
    "features_with_missing = data.columns[data.isnull().any()].tolist()\n",
    "if 'target' in features_with_missing:\n",
    "    features_with_missing.remove('target')\n",
    "\n",
    "print(f\"Features with missing values: {features_with_missing}\")\n",
    "\n",
    "# Missing value statistics\n",
    "missing_stats = []\n",
    "for feature in features_with_missing:\n",
    "    missing_count = data[feature].isnull().sum()\n",
    "    missing_pct = (missing_count / len(data)) * 100\n",
    "    missing_stats.append({\n",
    "        'Feature': feature,\n",
    "        'Missing Count': missing_count,\n",
    "        'Missing Percentage': missing_pct\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_stats)\n",
    "missing_df = missing_df.sort_values('Missing Percentage', ascending=False)\n",
    "print(\"\\nMissing Value Statistics:\")\n",
    "print(missing_df)\n",
    "\n",
    "# Visualize missing value patterns\n",
    "if len(features_with_missing) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Missing value heatmap\n",
    "    missing_matrix = data[features_with_missing].isnull()\n",
    "    sns.heatmap(missing_matrix.T, cbar=True, cmap='viridis', ax=axes[0])\n",
    "    axes[0].set_title('Missing Value Pattern\\n(Yellow = Missing, Purple = Present)')\n",
    "    axes[0].set_xlabel('Samples')\n",
    "    axes[0].set_ylabel('Features')\n",
    "    \n",
    "    # Missing value counts\n",
    "    missing_counts = missing_df.set_index('Feature')['Missing Count']\n",
    "    missing_counts.plot(kind='bar', ax=axes[1], color='orange')\n",
    "    axes[1].set_title('Missing Value Counts by Feature')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Missing value co-occurrence\n",
    "    if len(features_with_missing) > 1:\n",
    "        missing_corr = data[features_with_missing].isnull().corr()\n",
    "        sns.heatmap(missing_corr, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, fmt='.3f', ax=axes[2])\n",
    "        axes[2].set_title('Missing Value Correlation\\n(1 = Always missing together)')\n",
    "    \n",
    "    # Missing value impact on target\n",
    "    if len(features_with_missing) > 0:\n",
    "        missing_target_impact = []\n",
    "        for feature in features_with_missing:\n",
    "            missing_mask = data[feature].isnull()\n",
    "            if missing_mask.sum() > 0:\n",
    "                good_rate_missing = (data[missing_mask]['target'] == 1).mean()\n",
    "                good_rate_present = (data[~missing_mask]['target'] == 1).mean()\n",
    "                impact = good_rate_missing - good_rate_present\n",
    "                missing_target_impact.append({\n",
    "                    'Feature': feature.replace('_', ' ').title(),\n",
    "                    'Impact': impact\n",
    "                })\n",
    "        \n",
    "        if missing_target_impact:\n",
    "            impact_df = pd.DataFrame(missing_target_impact)\n",
    "            bars = axes[3].bar(range(len(impact_df)), impact_df['Impact'],\n",
    "                              color=['red' if x < 0 else 'green' for x in impact_df['Impact']])\n",
    "            axes[3].set_xticks(range(len(impact_df)))\n",
    "            axes[3].set_xticklabels(impact_df['Feature'], rotation=45, ha='right')\n",
    "            axes[3].set_ylabel('Difference in Good Customer Rate')\n",
    "            axes[3].set_title('Impact of Missing Values on Target\\n(+ means missing values → more good customers)')\n",
    "            axes[3].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, value in zip(bars, impact_df['Impact']):\n",
    "                height = bar.get_height()\n",
    "                axes[3].text(bar.get_x() + bar.get_width()/2.,\n",
    "                           height + (0.01 if height >= 0 else -0.01),\n",
    "                           f'{value:.3f}', ha='center', \n",
    "                           va='bottom' if height >= 0 else 'top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests for missing value impact\n",
    "print(\"Statistical Tests for Missing Value Impact on Target:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for feature in features_with_missing:\n",
    "    missing_mask = data[feature].isnull()\n",
    "    if missing_mask.sum() > 0 and missing_mask.sum() < len(data):\n",
    "        # Create contingency table\n",
    "        missing_target_crosstab = pd.crosstab(\n",
    "            data[feature].isnull(),\n",
    "            data['target'],\n",
    "            margins=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{feature.replace('_', ' ').title()}:\")\n",
    "        print(\"Contingency Table (Missing vs Target):\")\n",
    "        print(missing_target_crosstab)\n",
    "        \n",
    "        # Chi-square test\n",
    "        contingency_no_margins = missing_target_crosstab.iloc[:-1, :-1]\n",
    "        if contingency_no_margins.shape == (2, 2):\n",
    "            chi2, p_value, dof, expected = chi2_contingency(contingency_no_margins)\n",
    "            print(f\"Chi-square test: χ² = {chi2:.4f}, p-value = {p_value:.4f}\")\n",
    "            \n",
    "            # Calculate effect size (Phi coefficient for 2x2 tables)\n",
    "            n = contingency_no_margins.sum().sum()\n",
    "            phi = np.sqrt(chi2 / n)\n",
    "            print(f\"Effect size (Phi): {phi:.4f}\")\n",
    "            \n",
    "            # Interpretation\n",
    "            if p_value < 0.05:\n",
    "                print(\"*** Significant relationship between missing values and target ***\")\n",
    "            else:\n",
    "                print(\"No significant relationship between missing values and target\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Missing value pattern analysis\n",
    "if len(features_with_missing) > 1:\n",
    "    print(\"\\n\\nMissing Value Pattern Analysis:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Count unique missing patterns\n",
    "    missing_patterns = data[features_with_missing].isnull()\n",
    "    pattern_counts = missing_patterns.value_counts()\n",
    "    \n",
    "    print(f\"Number of unique missing patterns: {len(pattern_counts)}\")\n",
    "    print(\"\\nTop 10 missing patterns:\")\n",
    "    for i, (pattern, count) in enumerate(pattern_counts.head(10).items()):\n",
    "        pattern_str = \", \".join([f\"{feat}: {'Missing' if val else 'Present'}\" \n",
    "                                for feat, val in zip(features_with_missing, pattern)])\n",
    "        print(f\"{i+1:2d}. Count: {count:3d} | {pattern_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key insights\n",
    "print(\"KEY INSIGHTS AND RECOMMENDATIONS FOR CREDIT RISK ASSESSMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. COST-SENSITIVE NATURE:\")\n",
    "print(f\"   • 5:1 cost ratio makes False Negatives extremely expensive\")\n",
    "print(f\"   • Optimal threshold for 'good' classification: {break_even_threshold:.3f}\")\n",
    "print(f\"   • Always predicting 'bad' costs {n_good * 1} vs always 'good' costs {n_bad * 5}\")\n",
    "\n",
    "print(\"\\n2. CLASS IMBALANCE:\")\n",
    "print(f\"   • {n_good} good customers ({prior_good:.1%}) vs {n_bad} bad customers ({prior_bad:.1%})\")\n",
    "print(f\"   • Imbalance ratio: {imbalance_ratio:.2f}:1 (Good:Bad)\")\n",
    "print(f\"   • Consider cost-sensitive learning algorithms or sampling techniques\")\n",
    "\n",
    "print(\"\\n3. FEATURE IMPORTANCE (based on correlation and statistical tests):\")\n",
    "# Show top correlated features with target\n",
    "if 'target_correlations' in locals():\n",
    "    print(\"   Numerical features with strongest associations:\")\n",
    "    for feature, corr in target_correlations.head(3).items():\n",
    "        direction = \"increases\" if corr > 0 else \"decreases\"\n",
    "        print(f\"   • {feature.replace('_', ' ').title()}: {corr:.3f} ({direction} bad credit risk)\")\n",
    "\n",
    "print(\"\\n4. MISSING VALUES:\")\n",
    "if len(features_with_missing) > 0:\n",
    "    print(f\"   • {len(features_with_missing)} features have missing values\")\n",
    "    for feature in features_with_missing:\n",
    "        missing_pct = (data[feature].isnull().sum() / len(data)) * 100\n",
    "        print(f\"   • {feature.replace('_', ' ').title()}: {missing_pct:.1f}% missing\")\n",
    "    print(\"   • Consider imputation strategies or missing indicator variables\")\n",
    "else:\n",
    "    print(\"   • No missing values detected in the dataset\")\n",
    "\n",
    "print(\"\\n5. PREPROCESSING RECOMMENDATIONS:\")\n",
    "print(\"   • Handle missing values with domain-appropriate imputation\")\n",
    "print(\"   • Encode categorical variables (one-hot or ordinal encoding)\")\n",
    "print(\"   • Consider feature scaling for numerical variables\")\n",
    "print(\"   • Create interaction terms for strongly associated features\")\n",
    "\n",
    "print(\"\\n6. MODEL SELECTION RECOMMENDATIONS:\")\n",
    "print(\"   • Use cost-sensitive algorithms (e.g., cost-sensitive SVM, Random Forest)\")\n",
    "print(\"   • Consider ensemble methods with cost-sensitive learning\")\n",
    "print(\"   • Evaluate using cost-sensitive metrics, not just accuracy\")\n",
    "print(\"   • Use techniques like SMOTE with cost adjustment\")\n",
    "print(\"   • Cross-validation should maintain cost ratios\")\n",
    "\n",
    "print(\"\\n7. EVALUATION STRATEGY:\")\n",
    "print(\"   • Primary metric: Total cost (not accuracy)\")\n",
    "print(\"   • Secondary metrics: Precision/Recall with cost weighting\")\n",
    "print(\"   • ROC curves with cost-sensitive thresholds\")\n",
    "print(\"   • Business impact assessment (expected profit/loss)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE - Dataset ready for modeling phase\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}