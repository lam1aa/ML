{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Assessment - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs comprehensive EDA on the German Credit Risk dataset to understand:\n",
    "- Data structure and quality\n",
    "- Feature distributions\n",
    "- Target variable characteristics\n",
    "- Relationships between features and creditworthiness\n",
    "- Missing value patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('kredit.dat', sep='\\t', header=None)\n",
    "\n",
    "# Define feature names based on the problem description\n",
    "feature_names = [\n",
    "    'checking_account',      # A11-A14\n",
    "    'duration_months',       # numerical\n",
    "    'credit_history',        # A30-A34\n",
    "    'purpose',              # A40-A410 (incomplete)\n",
    "    'credit_amount',        # numerical\n",
    "    'savings_account',      # A61-A65\n",
    "    'employment_since',     # A71-A75 (incomplete)\n",
    "    'installment_rate',     # numerical\n",
    "    'personal_status',      # A91-A95\n",
    "    'other_debtors',        # A101-A103\n",
    "    'residence_since',      # numerical\n",
    "    'property',             # A121-A124\n",
    "    'age',                  # numerical\n",
    "    'other_installments',   # A141-A143\n",
    "    'housing',              # A151-A153\n",
    "    'existing_credits',     # numerical\n",
    "    'job',                  # A171-A175 (incomplete)\n",
    "    'num_dependents',       # numerical\n",
    "    'telephone',            # A191-A192\n",
    "    'foreign_worker',       # A201-A202 (incomplete)\n",
    "    'creditworthy'          # target: 1=yes, 2=no\n",
    "]\n",
    "\n",
    "data.columns = feature_names\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Features: {len(feature_names)-1}\")\n",
    "print(f\"Samples: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=== Dataset Info ===\")\n",
    "data.info()\n",
    "print(\"\\n=== First 5 rows ===\")\n",
    "display(data.head())\n",
    "print(\"\\n=== Last 5 rows ===\")\n",
    "display(data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values (marked as '?')\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Feature': data.columns,\n",
    "    'Missing_Count': [sum(data[col] == '?') for col in data.columns],\n",
    "    'Missing_Percentage': [sum(data[col] == '?') / len(data) * 100 for col in data.columns],\n",
    "    'Data_Type': data.dtypes\n",
    "})\n",
    "\n",
    "missing_analysis = missing_analysis.sort_values('Missing_Count', ascending=False)\n",
    "print(\"=== Missing Value Analysis ===\")\n",
    "display(missing_analysis[missing_analysis['Missing_Count'] > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Missing value counts\n",
    "missing_features = missing_analysis[missing_analysis['Missing_Count'] > 0]\n",
    "ax1.bar(missing_features['Feature'], missing_features['Missing_Count'])\n",
    "ax1.set_title('Missing Value Counts by Feature')\n",
    "ax1.set_ylabel('Count of Missing Values')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Missing value percentages\n",
    "ax2.bar(missing_features['Feature'], missing_features['Missing_Percentage'])\n",
    "ax2.set_title('Missing Value Percentages by Feature')\n",
    "ax2.set_ylabel('Percentage (%)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable\n",
    "target_counts = data['creditworthy'].value_counts().sort_index()\n",
    "target_percentages = data['creditworthy'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(\"=== Target Variable Analysis ===\")\n",
    "print(f\"Class 1 (Creditworthy): {target_counts[1]} ({target_percentages[1]:.1f}%)\")\n",
    "print(f\"Class 2 (Not Creditworthy): {target_counts[2]} ({target_percentages[2]:.1f}%)\")\n",
    "print(f\"Class Balance Ratio: {target_counts[1]/target_counts[2]:.2f}:1\")\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot\n",
    "labels = ['Creditworthy', 'Not Creditworthy']\n",
    "colors = ['lightgreen', 'lightcoral']\n",
    "ax1.bar(labels, target_counts.values, color=colors)\n",
    "ax1.set_title('Target Variable Distribution')\n",
    "ax1.set_ylabel('Count')\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    ax1.text(i, v + 10, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(target_counts.values, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Target Variable Proportion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cost implication analysis\n",
    "print(\"\\n=== Cost Implication ===\")\n",
    "print(\"Given 5:1 cost ratio (FP:FN):\")\n",
    "print(f\"- Misclassifying {target_counts[2]} non-creditworthy as creditworthy = {target_counts[2] * 5} cost units\")\n",
    "print(f\"- Misclassifying {target_counts[1]} creditworthy as non-creditworthy = {target_counts[1] * 1} cost units\")\n",
    "print(f\"- Total potential cost if all wrong: {target_counts[2] * 5 + target_counts[1] * 1} units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical features\n",
    "numerical_features = ['duration_months', 'credit_amount', 'installment_rate', \n",
    "                     'residence_since', 'age', 'existing_credits', 'num_dependents']\n",
    "\n",
    "# Convert to numeric (handling any string values)\n",
    "for col in numerical_features:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Statistical summary\n",
    "print(\"=== Numerical Features Statistical Summary ===\")\n",
    "display(data[numerical_features].describe())\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numerical_features):\n",
    "    if i < len(axes):\n",
    "        axes[i].hist(data[col].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[i].set_title(f'{col.replace(\"_\", \" \").title()} Distribution')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(numerical_features) < len(axes):\n",
    "    for i in range(len(numerical_features), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical features\n",
    "categorical_features = [col for col in data.columns if col not in numerical_features + ['creditworthy']]\n",
    "\n",
    "print(f\"=== Categorical Features ({len(categorical_features)}) ===\")\n",
    "print(categorical_features)\n",
    "\n",
    "# Analyze each categorical feature\n",
    "for feature in categorical_features:\n",
    "    print(f\"\\n--- {feature.upper()} ---\")\n",
    "    value_counts = data[feature].value_counts()\n",
    "    print(f\"Unique values: {len(value_counts)}\")\n",
    "    print(value_counts)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_count = sum(data[feature] == '?')\n",
    "    if missing_count > 0:\n",
    "        print(f\"Missing values ('?'): {missing_count} ({missing_count/len(data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical features\n",
    "n_categorical = len(categorical_features)\n",
    "n_cols = 3\n",
    "n_rows = (n_categorical + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    if i < len(axes):\n",
    "        value_counts = data[feature].value_counts()\n",
    "        axes[i].bar(range(len(value_counts)), value_counts.values, \n",
    "                   color='lightsteelblue', edgecolor='black')\n",
    "        axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "        axes[i].set_xlabel('Categories')\n",
    "        axes[i].set_ylabel('Count')\n",
    "        axes[i].set_xticks(range(len(value_counts)))\n",
    "        axes[i].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(n_categorical, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature-Target Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features vs target\n",
    "print(\"=== Numerical Features vs Target ===\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    if i < len(axes):\n",
    "        # Box plot\n",
    "        creditworthy_data = data[data['creditworthy'] == 1][feature].dropna()\n",
    "        not_creditworthy_data = data[data['creditworthy'] == 2][feature].dropna()\n",
    "        \n",
    "        axes[i].boxplot([creditworthy_data, not_creditworthy_data], \n",
    "                       labels=['Creditworthy', 'Not Creditworthy'])\n",
    "        axes[i].set_title(f'{feature.replace(\"_\", \" \").title()} by Creditworthiness')\n",
    "        axes[i].set_ylabel(feature)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(numerical_features) < len(axes):\n",
    "    for i in range(len(numerical_features), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests for numerical features\n",
    "print(\"\\n=== Statistical Tests (t-test) for Numerical Features ===\")\n",
    "for feature in numerical_features:\n",
    "    group1 = data[data['creditworthy'] == 1][feature].dropna()\n",
    "    group2 = data[data['creditworthy'] == 2][feature].dropna()\n",
    "    \n",
    "    if len(group1) > 0 and len(group2) > 0:\n",
    "        statistic, p_value = stats.ttest_ind(group1, group2)\n",
    "        print(f\"{feature}: t-statistic = {statistic:.4f}, p-value = {p_value:.6f}\")\n",
    "        print(f\"  Mean Creditworthy: {group1.mean():.2f}\")\n",
    "        print(f\"  Mean Not Creditworthy: {group2.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs target\n",
    "print(\"=== Categorical Features vs Target ===\")\n",
    "\n",
    "# Create cross-tabulations and visualizations\n",
    "n_categorical = len(categorical_features)\n",
    "n_cols = 2\n",
    "n_rows = (n_categorical + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 6*n_rows))\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    if i < len(axes):\n",
    "        # Create crosstab\n",
    "        crosstab = pd.crosstab(data[feature], data['creditworthy'])\n",
    "        \n",
    "        # Normalized crosstab for proportions\n",
    "        crosstab_norm = pd.crosstab(data[feature], data['creditworthy'], normalize='index') * 100\n",
    "        \n",
    "        # Plot stacked bar chart\n",
    "        crosstab_norm.plot(kind='bar', stacked=True, ax=axes[i], \n",
    "                          color=['lightgreen', 'lightcoral'])\n",
    "        axes[i].set_title(f'{feature.replace(\"_\", \" \").title()} vs Creditworthiness')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Percentage')\n",
    "        axes[i].legend(['Creditworthy', 'Not Creditworthy'])\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(n_categorical, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square tests for categorical features\n",
    "print(\"\\n=== Chi-Square Tests for Categorical Features ===\")\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi_square_results = []\n",
    "\n",
    "for feature in categorical_features:\n",
    "    # Create contingency table\n",
    "    contingency_table = pd.crosstab(data[feature], data['creditworthy'])\n",
    "    \n",
    "    # Perform chi-square test\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    chi_square_results.append({\n",
    "        'Feature': feature,\n",
    "        'Chi2_Statistic': chi2,\n",
    "        'P_Value': p_value,\n",
    "        'Degrees_of_Freedom': dof,\n",
    "        'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "    })\n",
    "    \n",
    "    print(f\"{feature}: χ² = {chi2:.4f}, p-value = {p_value:.6f}, significant = {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "chi_square_df = pd.DataFrame(chi_square_results)\n",
    "chi_square_df = chi_square_df.sort_values('P_Value')\n",
    "print(\"\\n=== Summary of Chi-Square Tests ===\")\n",
    "display(chi_square_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "print(\"=== Correlation Analysis for Numerical Features ===\")\n",
    "numerical_data = data[numerical_features + ['creditworthy']].copy()\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = numerical_data.corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title('Correlation Matrix - Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlations with target\n",
    "target_correlations = correlation_matrix['creditworthy'].drop('creditworthy').sort_values(key=abs, ascending=False)\n",
    "print(\"\\n=== Correlations with Target Variable (sorted by absolute value) ===\")\n",
    "for feature, corr in target_correlations.items():\n",
    "    print(f\"{feature}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for key numerical features\n",
    "print(\"=== Pairplot for Key Numerical Features ===\")\n",
    "key_features = ['duration_months', 'credit_amount', 'age', 'creditworthy']\n",
    "pairplot_data = data[key_features].copy()\n",
    "pairplot_data['creditworthy'] = pairplot_data['creditworthy'].map({1: 'Creditworthy', 2: 'Not Creditworthy'})\n",
    "\n",
    "sns.pairplot(pairplot_data, hue='creditworthy', diag_kind='hist', \n",
    "             palette={'Creditworthy': 'lightgreen', 'Not Creditworthy': 'lightcoral'})\n",
    "plt.suptitle('Pairplot of Key Numerical Features by Creditworthiness', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance based on mutual information\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"=== Feature Importance Analysis ===\")\n",
    "\n",
    "# Prepare data for mutual information calculation\n",
    "X_encoded = data.drop('creditworthy', axis=1).copy()\n",
    "y = data['creditworthy'].copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    # Handle missing values by treating them as a separate category\n",
    "    X_encoded[col] = X_encoded[col].astype(str)\n",
    "    X_encoded[col] = le.fit_transform(X_encoded[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Calculate mutual information\n",
    "mi_scores = mutual_info_classif(X_encoded, y, random_state=42)\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_encoded.columns,\n",
    "    'Mutual_Information': mi_scores\n",
    "}).sort_values('Mutual_Information', ascending=False)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=feature_importance.head(15), x='Mutual_Information', y='Feature')\n",
    "plt.title('Top 15 Features by Mutual Information Score')\n",
    "plt.xlabel('Mutual Information Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Top 10 Features by Mutual Information ===\")\n",
    "display(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== EDA SUMMARY AND INSIGHTS ===\")\n",
    "print(\"\\n1. DATASET OVERVIEW:\")\n",
    "print(f\"   - Total samples: {len(data)}\")\n",
    "print(f\"   - Total features: {len(data.columns) - 1}\")\n",
    "print(f\"   - Numerical features: {len(numerical_features)}\")\n",
    "print(f\"   - Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "print(\"\\n2. TARGET VARIABLE:\")\n",
    "print(f\"   - Creditworthy (1): {target_counts[1]} ({target_percentages[1]:.1f}%)\")\n",
    "print(f\"   - Not Creditworthy (2): {target_counts[2]} ({target_percentages[2]:.1f}%)\")\n",
    "print(f\"   - Class imbalance ratio: {target_counts[1]/target_counts[2]:.2f}:1\")\n",
    "\n",
    "print(\"\\n3. MISSING VALUES:\")\n",
    "missing_features_summary = missing_analysis[missing_analysis['Missing_Count'] > 0]\n",
    "if len(missing_features_summary) > 0:\n",
    "    for _, row in missing_features_summary.iterrows():\n",
    "        print(f\"   - {row['Feature']}: {row['Missing_Count']} ({row['Missing_Percentage']:.1f}%)\")\nelse:\n    print(\"   - No missing values found\")\n\nprint(\"\\n4. KEY STATISTICAL INSIGHTS:\")\nprint(\"   Numerical Features:\")\nfor feature in numerical_features:\n    group1 = data[data['creditworthy'] == 1][feature].dropna()\n    group2 = data[data['creditworthy'] == 2][feature].dropna()\n    if len(group1) > 0 and len(group2) > 0:\n        diff_pct = ((group1.mean() - group2.mean()) / group2.mean()) * 100\n        print(f\"   - {feature}: Creditworthy avg = {group1.mean():.2f}, Not creditworthy avg = {group2.mean():.2f} (diff: {diff_pct:+.1f}%)\")\n\nprint(\"\\n5. MOST IMPORTANT FEATURES (by Mutual Information):\")\nfor i, (_, row) in enumerate(feature_importance.head(5).iterrows()):\n    print(f\"   {i+1}. {row['Feature']}: {row['Mutual_Information']:.4f}\")\n\nprint(\"\\n6. PREPROCESSING RECOMMENDATIONS:\")\nprint(\"   - Handle missing values in 4 features (create 'unknown' category or impute)\")\nprint(\"   - Consider log transformation for credit_amount (likely skewed)\")\nprint(\"   - One-hot encode categorical features\")\nprint(\"   - Apply cost-sensitive learning (5:1 FP:FN cost ratio)\")\nprint(\"   - Consider feature scaling for distance-based algorithms\")\nprint(\"   - Address class imbalance if necessary\")\n\nprint(\"\\n7. MODELING CONSIDERATIONS:\")\nprint(\"   - Use stratified sampling for train-test split\")\nprint(\"   - Implement cost-sensitive evaluation metrics\")\nprint(\"   - Consider ensemble methods for handling feature interactions\")\nprint(\"   - Validate model performance using cost-weighted metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 }
}